{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium 사용 실습(1)\n",
    "-설정 및 테스트<br>\n",
    "-드라이브설치<br>\n",
    "-웹자동화의 이해<br>\n",
    "-selenium 기초학습<br>\n",
    "-다음사이트기반 학습<br><br>\n",
    "<br>\n",
    "셀레니움 사용 이유? <br><br>\n",
    "후처리되는 랜더링을 통해 정보를 표시해주는 서버프로그래밍이 되어 있는 사이트<br>\n",
    "실질적인 브라우저인지 체킹하는 보안이 되어있는 사이트<br>\n",
    "엔터키를 포함한 키보드의 모든 키 사용 , 마우스 휠 등 <br>\n",
    "자바스크립트 실행<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenum 임포트\n",
    "\n",
    "from selenium import webdriver\n",
    "\n",
    "# webdriver 설정(Chrome , Firefox 등)\n",
    "\n",
    "browser = webdriver.Chrome('C:\\\\chromedriver\\\\chromedriver.exe')\n",
    "\n",
    "# 크롬브라우저 내부 대기\n",
    "#컴퓨터 사양을 타기때문에 브라우저에 time을 걸어준다.\n",
    "browser.implicitly_wait(5)\n",
    "\n",
    "\n",
    "#속성확인\n",
    "print(dir(browser))\n",
    "\n",
    "\n",
    "#브라우저 사이즈\n",
    "browser.set_window_size(1920,1280) # maximize_window(),minimize_window()\n",
    "\n",
    "\n",
    "# 페이지 이동\n",
    "browser.get('https://www.daum.net')\n",
    "\n",
    "\n",
    "# 페이지 내용\n",
    "# print('Pager Contents : {}'.format(browser.page_source))\n",
    "\n",
    "\n",
    "#세션 값 출력 \n",
    "# 브라우저 엔진을 가지고 접근하기때문에 세션쿠키를 다 가지고 있음\n",
    "# 서버자체에서는 브라우저 접근이라고 인식\n",
    "# 하지만 user Agent에 selenium이라고 뜨기때문에 그것조차 막아놓은 사이트가 있을 수 있음 (10%정도)\n",
    "print('Session ID : {}'.format(browser.session_id))\n",
    "\n",
    "\n",
    "#타이틀 출력\n",
    "print('Title : {}'.format(browser.title))\n",
    "\n",
    "\n",
    "# 현재 URL출력\n",
    "print('URL :{}'.format(browser.current_url))\n",
    "\n",
    "\n",
    "# 현재 쿠키 정보 출력 \n",
    "# 크롬개발자도구-application-쿠키)\n",
    "print('Cookies : {}'.format(browser.get_cookies()))\n",
    "\n",
    "#검색창 input 선택\n",
    "element = browser.find_element_by_css_selector('div.inner_search > input.tf_keyword')\n",
    "\n",
    "#검색어 입력\n",
    "element.send_keys('스튜디오드래곤')\n",
    "\n",
    "#검색(Form Submit)\n",
    "element.submit()\n",
    "\n",
    "\n",
    "# 스크린 샷 저장1 ( 코드 점검용으로 실행된다. ,  실행비용이 많이드는 작업이므로 코드생성시에만 실행하고 생성완료후 주석처리 )\n",
    "# browser.save_screenshot(\"C:/website_ch1.jpg\")\n",
    "# 스크린 샷 저장2\n",
    "# browser.get_screenshot_as_file(\"C:/website.ch2.jpg\")\n",
    "\n",
    "\n",
    "#브라우저 종료  ( 리소스 낭비하지 않기하게 위해 끄기)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Selenium 사용 실습(2)  -  실습 프로젝트(1)\n",
    "\n",
    "\n",
    "* 대상사이트 선정 및 분석\n",
    "* explicitly wait\n",
    "* implicitly wait\n",
    "* 필요정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url은 변하지 않고 자바스크립트 호출로 인하여 데이터가 변경되는 경우(리랜더링되는 경우) <br>\n",
    "selenium을 쓰지않고 스크래핑을 할수 있다 하지만 이렇게 코드를 생산할 경우 비교적 긴 코드를 작성해야하고, <br>\n",
    "변경사항이 있을 경우 수정해야될 필요가 있다.\n",
    "\n",
    "selenium 역시 변경사항이 있을 경우가 있지만 css선택자만 변경하는 것처럼 간단한 수정으로 유지보수가 가능하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델명  , 시기  , 가격"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver                    \n",
    "import time                                                        # 시간이나 슬립줄때 사용\n",
    "from selenium.webdriver.common.by import By                       #   ~까지 wait줄때 사용 \n",
    "from selenium.webdriver.support.ui import WebDriverWait           # 브라우저 로딩을 할때까지 사용 by와 같이 사용된다\n",
    "from selenium.webdriver.support import expected_conditions as EC  # 어떤상태가 될때까지 \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 크롬브라우져를 실행하지않고 내부적으로 동작\n",
    "\n",
    "\n",
    "# webdriver 설정(chrome,Firefox 등) - headless 모드  : 코드작성이 완료되었다면 리소스절약을 위해서\n",
    "browser = webdriver.Chrome('C:\\\\chromedriver\\\\chromedriver.exe',options=chrome_options)\n",
    "\n",
    "\n",
    "# webdriver 설정(chrome,Firefox 등) - 일반 모드 \n",
    "# browser = webdriver.Chrome('C:\\\\chromedriver\\\\chromedriver.exe')\n",
    "\n",
    "\n",
    "# 크롬 브라우저 내부 대기\n",
    "browser.implicitly_wait(5)  #약간은 확실하지않은 방법\n",
    "\n",
    "\n",
    "# 브라우저 사이즈 ( 노트북모니터가 아닌 일반 모니터 좀 더 큰 값으로 )\n",
    "browser.set_window_size(1620,1080)  #maximize_window(),  minimize_window()\n",
    "\n",
    "\n",
    "# 페이지\n",
    "browser.get('http://prod.danawa.com/list/?cate=112758&15main_11_02')\n",
    "\n",
    "# 1차 페이지 내용\n",
    "# print('Before Page Contents: {}'.format(browser.page_source))\n",
    "\n",
    "\n",
    "#제조사별 더 보기\n",
    "# 클릭할 버튼이 아직 랜더링되지않았는데 컴퓨터가 click을 스스로 한다면 에러가 발생할 수 있다.\n",
    "# 좀더 명시적인 기다림을 위해 explicitly 사용\n",
    "# 3초동안 해당 xpath를 찾지못하면 에러를 발생\n",
    "WebDriverWait(browser,3).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"dlMaker_simple\"]/dd/div[2]/button[1]'))).click()\n",
    "\n",
    "\n",
    "#implicitly wait\n",
    "# time.sleep(2)   #interpreter 엔진 전체가 멈추는 것이기 때문에 권장하지는 않음\n",
    "# browser.find_element_by_xpath('//*[@id=\"dlMaker_simple\"]/dd/div[2]/button[1]').click()\n",
    "\n",
    "\n",
    "# 원하는 모델 카테고리 클릭\n",
    "WebDriverWait(browser,2).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"selectMaker_simple_priceCompare_A\"]/li[13]/label'))).click()\n",
    "\n",
    "\n",
    "# 2차 페이지 내용\n",
    "# print('After Page Contents: {}'.format(browser.page_source))\n",
    "\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# bs4 초기화\n",
    "soup = BeautifulSoup(browser.page_source,'html.parser')\n",
    "\n",
    "\n",
    "# 소스코드 정리\n",
    "# print(soup.prettify())\n",
    "\n",
    "# 메인 상품 리스트 선택\n",
    "pro_list = soup.select('div.main_prodlist.main_prodlist_list > ul > li')\n",
    "\n",
    "# print(pro_list)t\n",
    "\n",
    "\n",
    "# 필요 정보 추출\n",
    "for v in pro_list:\n",
    "    #임시출력\n",
    "#     print(v)\n",
    "    if not v.find('div',class_='ad_header'):\n",
    "        \n",
    "        # 상품명, 이미지, 가격\n",
    "        print(v.select('p.prod_name > a')[0].text.strip())\n",
    "        try:\n",
    "            print(v.select('a.thumb_link > img')[0]['data-original'])\n",
    "        except:\n",
    "            print(v.select('a.thumb_link > img')[0]['src'])\n",
    "        print(v.select('p.price_sect > a')[0].text.strip())\n",
    "    print()\n",
    "\n",
    "         \n",
    "# 브라우저 종료\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (다나와 사이트) 데이터 수집 프로젝트 생성\n",
    "* 페이지 전환 추가\n",
    "* Selenium 성능 개선\n",
    "* 전체 프로세스 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver                    \n",
    "import time                                                        # 시간이나 슬립줄때 사용\n",
    "from selenium.webdriver.common.by import By                       #   ~까지 wait줄때 사용 \n",
    "from selenium.webdriver.support.ui import WebDriverWait           # 브라우저 로딩을 할때까지 사용 by와 같이 사용된다\n",
    "from selenium.webdriver.support import expected_conditions as EC  # 어떤상태가 될때까지 \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # 크롬브라우져를 실행하지않고 내부적으로 동작\n",
    "\n",
    "\n",
    "    # webdriver 설정(chrome,Firefox 등) - headless 모드  : 코드작성이 완료되었다면 리소스절약을 위해서\n",
    "    browser = webdriver.Chrome('C:\\\\chromedriver\\\\chromedriver.exe',options=chrome_options)\n",
    "\n",
    "\n",
    "    # webdriver 설정(chrome,Firefox 등) - 일반 모드 \n",
    "    # browser = webdriver.Chrome('C:\\\\chromedriver\\\\chromedriver.exe')\n",
    "\n",
    "\n",
    "    # 크롬 브라우저 내부 대기\n",
    "    browser.implicitly_wait(5)  #약간은 확실하지않은 방법\n",
    "\n",
    "\n",
    "    # 브라우저 사이즈 ( 노트북모니터가 아닌 일반 모니터 좀 더 큰 값으로 )\n",
    "    browser.set_window_size(1620,1080)  #maximize_window(),  minimize_window()\n",
    "\n",
    "\n",
    "    # 페이지\n",
    "    browser.get('http://prod.danawa.com/list/?cate=112758&15main_11_02')\n",
    "\n",
    "    # 1차 페이지 내용\n",
    "    # print('Before Page Contents: {}'.format(browser.page_source))\n",
    "\n",
    "\n",
    "    #제조사별 더 보기\n",
    "    # 클릭할 버튼이 아직 랜더링되지않았는데 컴퓨터가 click을 스스로 한다면 에러가 발생할 수 있다.\n",
    "    # 좀더 명시적인 기다림을 위해 explicitly 사용\n",
    "    # 3초동안 해당 xpath를 찾지못하면 에러를 발생\n",
    "    WebDriverWait(browser,3).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"dlMaker_simple\"]/dd/div[2]/button[1]'))).click()\n",
    "\n",
    "\n",
    "\n",
    "    # 원하는 모델 카테고리 클릭\n",
    "    WebDriverWait(browser,2).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"selectMaker_simple_priceCompare_A\"]/li[13]/label'))).click()\n",
    "\n",
    "\n",
    "\n",
    "    # 2초간 대기 : 컴퓨터 속도에 따라 다르게 설정\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 현재페이지\n",
    "    cur_page = 1\n",
    "\n",
    "    # 크롤링 페이지 수\n",
    "    target_crawl_num = 5\n",
    "    # 페이지의 끝을 모를 경우  조건을 주어서 다음 숫자가 없는 경우 마지막페이지를 식별한다.\n",
    "    # 코드의 수정이 빈번한 경우를 대비해서 어느정도의 하드코딩이 필요할 수 있다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    while cur_page <= target_crawl_num:\n",
    "\n",
    "        # bs4 초기화\n",
    "        soup = BeautifulSoup(browser.page_source,'html.parser')\n",
    "\n",
    "\n",
    "        # 소스코드 정리\n",
    "        # print(soup.prettify())\n",
    "\n",
    "        # 메인 상품 리스트 선택\n",
    "        pro_list = soup.select('div.main_prodlist.main_prodlist_list > ul > li')\n",
    "\n",
    "        # 페이지 번호 출력\n",
    "        print(\"******** current page : {}\".format(cur_page), \"*********\")\n",
    "        print()\n",
    "\n",
    "        # 필요 정보 추출\n",
    "        for v in pro_list:\n",
    "            #임시출력\n",
    "        #     print(v)\n",
    "            if not v.find('div',class_='ad_header'):\n",
    "\n",
    "                # 상품명, 이미지, 가격\n",
    "                print(v.select('p.prod_name > a')[0].text.strip())\n",
    "\n",
    "                try:\n",
    "                    print(v.select('a.thumb_link > img')[0]['data-original'])\n",
    "                except:\n",
    "                    print(v.select('a.thumb_link > img')[0]['src'])\n",
    "\n",
    "                print(v.select('p.price_sect > a')[0].text.strip())\n",
    "\n",
    "\n",
    "                # 이부분에서 엑셀 저장(파일,DB 등)\n",
    "                # CODE\n",
    "                # CODE   \n",
    "\n",
    "            print()        \n",
    "        print()  \n",
    "\n",
    "        # 페이지별 스크린 샷 저장\n",
    "        browser.save_screenshot(\"C:/target_page{}.png\".format(cur_page))\n",
    "\n",
    "        #페이지 증가\n",
    "        cur_page += 1\n",
    "\n",
    "\n",
    "        if cur_page > target_crawl_num:\n",
    "            print('Crawling Succeed!')\n",
    "            break\n",
    "\n",
    "\n",
    "        #페이지 이동 클릭\n",
    "        WebDriverWait(browser,2).until(EC.presence_of_element_located((By.CSS_SELECTOR,'div.number_wrap > a:nth-child({})'.format(cur_page)))).click()\n",
    "\n",
    "\n",
    "        # BeautifulSoup 인스턴스 메모리에서 삭제  :사용하지않는 영역에 저장된 리소스 ,garbage collector를 삭제 , 튜닝, 성능적으로 안정적인 코딩\n",
    "        del soup\n",
    "\n",
    "        # 랜더링 시간 3초간 대기\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "    # 브라우저 종료\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엑셀 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "엑셀파일을 만들어주는 패키지 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter  \n",
    "from io import BytesIO # 이미지 바이트 처리 \n",
    "\n",
    "import urllib.request as req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 크롬브라우져를 실행하지않고 내부적으로 동작\n",
    "\n",
    "\n",
    "# 엑셀 처리 선언\n",
    "workbook = xlsxwriter.Workbook(\"C:/crawling_result.xlsx\")\n",
    "\n",
    "# 워크 시트\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "\n",
    "# webdriver 설정(chrome,Firefox 등) - headless 모드  : 코드작성이 완료되었다면 리소스절약을 위해서\n",
    "browser = webdriver.Chrome('C:\\\\chromedriver\\\\chromedriver.exe',options=chrome_options)\n",
    "\n",
    "\n",
    "# webdriver 설정(chrome,Firefox 등) - 일반 모드 \n",
    "# browser = webdriver.Chrome('C:\\\\chromedriver\\\\chromedriver.exe')\n",
    "\n",
    "\n",
    "# 크롬 브라우저 내부 대기\n",
    "browser.implicitly_wait(5)  #약간은 확실하지않은 방법\n",
    "\n",
    "\n",
    "# 브라우저 사이즈 ( 노트북모니터가 아닌 일반 모니터 좀 더 큰 값으로 )\n",
    "browser.set_window_size(1620,1080)  #maximize_window(),  minimize_window()\n",
    "\n",
    "\n",
    "# 페이지\n",
    "browser.get('http://prod.danawa.com/list/?cate=112758&15main_11_02')\n",
    "\n",
    "# 1차 페이지 내용\n",
    "# print('Before Page Contents: {}'.format(browser.page_source))\n",
    "\n",
    "\n",
    "#제조사별 더 보기\n",
    "# 클릭할 버튼이 아직 랜더링되지않았는데 컴퓨터가 click을 스스로 한다면 에러가 발생할 수 있다.\n",
    "# 좀더 명시적인 기다림을 위해 explicitly 사용\n",
    "# 3초동안 해당 xpath를 찾지못하면 에러를 발생\n",
    "WebDriverWait(browser,3).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"dlMaker_simple\"]/dd/div[2]/button[1]'))).click()\n",
    "\n",
    "\n",
    "\n",
    "# 원하는 모델 카테고리 클릭\n",
    "WebDriverWait(browser,2).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"selectMaker_simple_priceCompare_A\"]/li[13]/label'))).click()\n",
    "\n",
    "\n",
    "\n",
    "# 2초간 대기 : 컴퓨터 속도에 따라 다르게 설정\n",
    "time.sleep(2)\n",
    "\n",
    "# 현재페이지\n",
    "cur_page = 1\n",
    "\n",
    "# 크롤링 페이지 수\n",
    "target_crawl_num = 5\n",
    "# 페이지의 끝을 모를 경우  조건을 주어서 다음 숫자가 없는 경우 마지막페이지를 식별한다.\n",
    "# 코드의 수정이 빈번한 경우를 대비해서 어느정도의 하드코딩이 필요할 수 있다.\n",
    "\n",
    "# 엑셀 행 수(1부터 시작)\n",
    "ins_cnt = 1\n",
    "\n",
    "\n",
    "while cur_page <= target_crawl_num:\n",
    "\n",
    "    # bs4 초기화\n",
    "    soup = BeautifulSoup(browser.page_source,'html.parser')\n",
    "\n",
    "\n",
    "    # 소스코드 정리\n",
    "    # print(soup.prettify())\n",
    "\n",
    "    # 메인 상품 리스트 선택\n",
    "    pro_list = soup.select('div.main_prodlist.main_prodlist_list > ul > li')\n",
    "\n",
    "    # 페이지 번호 출력\n",
    "    print(\"******** current page : {}\".format(cur_page), \"*********\")\n",
    "    print()\n",
    "\n",
    "    # 필요 정보 추출\n",
    "    for v in pro_list:\n",
    "        #임시출력\n",
    "    #     print(v)\n",
    "        if not v.find('div',class_='ad_header'):\n",
    "            \n",
    "            prod_name = v.select('p.prod_name > a')[0].text.strip()\n",
    "            \n",
    "            prod_price = v.select('p.price_sect > a')[0].text.strip()\n",
    "          \n",
    "        \n",
    "            #이미지 요청 후 바이트 변환   / gif 처리 할것...\n",
    "            try:\n",
    "                try:\n",
    "                    img_data = BytesIO(req.urlopen(v.select('a.thumb_link > img')[0]['data-original']).read())\n",
    "                except:\n",
    "                    img_data = BytesIO(req.urlopen(v.select('a.thumb_link > img')[0]['src']).read())\n",
    "            except:\n",
    "                print(v)\n",
    "                raise Exception\n",
    "            # 엑셀 텍스트 저장\n",
    "            worksheet.write('A%s'% ins_cnt , prod_name )            \n",
    "            worksheet.write('B%s'% ins_cnt , prod_price )\n",
    "            \n",
    "            # 엑셀 이미지 저장\n",
    "            worksheet.insert_image('C%s'% ins_cnt , prod_name , {'image_data':img_data} )\n",
    "            \n",
    "            \n",
    "            ins_cnt += 1\n",
    "            \n",
    "        print()        \n",
    "    print()  \n",
    "    \n",
    "    # 페이지별 스크린 샷 저장\n",
    "    browser.save_screenshot(\"C:/target_page{}.png\".format(cur_page))\n",
    "    \n",
    "    #페이지 증가\n",
    "    cur_page += 1\n",
    "    \n",
    "    \n",
    "    if cur_page > target_crawl_num:\n",
    "        print('Crawling Succeed!')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    #페이지 이동 클릭\n",
    "    WebDriverWait(browser,2).until(EC.presence_of_element_located((By.CSS_SELECTOR,'div.number_wrap > a:nth-child({})'.format(cur_page)))).click()\n",
    "\n",
    "    \n",
    "    # BeautifulSoup 인스턴스 메모리에서 삭제  :사용하지않는 영역에 저장된 리소스 ,garbage collector를 삭제 , 튜닝, 성능적으로 안정적인 코딩\n",
    "    del soup\n",
    "\n",
    "    # 랜더링 시간 3초간 대기\n",
    "    time.sleep(3)\n",
    "    \n",
    "         \n",
    "# 브라우저 종료\n",
    "browser.close()\n",
    "\n",
    "# 엑셀 파일 닫기\n",
    "workbook.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

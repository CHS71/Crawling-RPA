{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Requests 요청정보 Payload\n",
    "\n",
    "* 세션 활성화 및 비활성화\n",
    "* 쿠키 정보 전송\n",
    "* User-Agent 정보 전송\n",
    "* 수신 상태 코드 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 세션 활성화\n",
    "s = requests.Session()\n",
    "r = s.get('https://www.naver.com')\n",
    "\n",
    "# 수신 데이터\n",
    "# print(r.text)\n",
    "\n",
    "\n",
    "#  수신 상태 코드\n",
    "print('Status Code: {}'.format(r.status_code))\n",
    "\n",
    "\n",
    "# 확인\n",
    "print('Ok? : {}'.format(r.ok))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 쿠기 Return\n",
    "\n",
    "r = s.get('http://httpbin.org/cookies',cookies = {'name':'kim'})\n",
    "\n",
    "print(r.text)\n",
    "\n",
    "\n",
    "# 쿠키 set (rest API, 서버쪽에 저장)\n",
    "\n",
    "r2 = s.get('http://httpbin.org/cookies/set',cookies={'name':'kim2'})\n",
    "print(r2.text)\n",
    "\n",
    "# User-Agent\n",
    "url = 'http://httpbin.org'\n",
    "headers = {'user-agent':'nice-man_1.0.0_win10_ram16_home'}\n",
    "\n",
    "\n",
    "\n",
    "# Header 정보 전송\n",
    "r3 = s.get(url, headers=headers)\n",
    "print(r3.text)\n",
    "\n",
    "\n",
    "# 세션 비활성화\n",
    "s.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with문 사용(권장) -> 파일, DB , HTTP\n",
    "with requests.Session()  as s:\n",
    "    r = s.get('https://www.daum.net')\n",
    "    print(r.text)\n",
    "    print(r.ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Httpbin 사이트를 이용한 JSON 실습\n",
    "\n",
    "* 수신데이터 처리 실습\n",
    "* 수신 데이터 -> JSON 변환 출력\n",
    "* Response 다양한 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "s = requests.Session() \n",
    "\n",
    "# 100개 JSON 데이터 요청\n",
    "# text 형태의 데이터를 불러올때는 stream값을 True로 하는것이 좋다고만 알고있으면 된다. (내부적으로 stream형태로 가져옴)\n",
    "r = s.get('https://httpbin.org/stream/100', stream= True)\n",
    "\n",
    "# 수신 확인\n",
    "# print(r.text)\n",
    "\n",
    "\n",
    "# Encoding 확인\n",
    "# print('Before Encoding : {}'.format(r.encoding))\n",
    "\n",
    "\n",
    "# encoding이 없을 경우 예외처리\n",
    "if r.encoding is None:\n",
    "    r.encoding = 'UTF-8'\n",
    "    \n",
    "print('After Encoding : {}'.format(r.encoding))\n",
    "\n",
    "\n",
    "\n",
    "for line in r.iter_lines(decode_unicode=True): # 깨짐방지\n",
    "    #라인 출력 후 타입 확인\n",
    "#     print(line)\n",
    "#     print(type(line))\n",
    "\n",
    "    # JSON(Dict) 변환 후 타입 확인\n",
    "    b = json.loads(line) # str -> dict\n",
    "#     print(b)\n",
    "#     print(type(b))\n",
    "\n",
    "\n",
    "    # 정보 내용 출력\n",
    "    for k,v in b.items():\n",
    "        print(\"key : {} , value : {}\".format(k,v))\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "\n",
    "\n",
    "s.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = s.get('https://jsonplaceholder.typicode.com/todos/1')\n",
    "\n",
    "# Header정보\n",
    "print(r.headers)\n",
    "\n",
    "#'Content-Type': 'application/json;\n",
    "\n",
    "\n",
    "# 본문 정보\n",
    "# print(r.text)\n",
    "\n",
    "# JSON 변환\n",
    "print(r.json())\n",
    "\n",
    "# key 변환\n",
    "print(r.json().keys())\n",
    "\n",
    "# values 변환\n",
    "print(r.json().values())\n",
    "\n",
    "# 인코딩 반환\n",
    "print(r.encoding)\n",
    "\n",
    "# 바이너리 정보\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 개발자 도구 송수신 분석 및 실습\n",
    "\n",
    "* Rest API란?\n",
    "* POST , PUT\n",
    "* DELETE\n",
    "* Requests 최종 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rest API : GET, POST, DELETE, PUT:UPDATE, REPLACE(FETCH : UPDATE, MODIFY)\n",
    "# 중요 :  URL을 활용해서 자원의 상태 정보를 주고 받는 모든 것을 의미  , URL로 자원의 행동을 실행시킬 수 있는 것.\n",
    "# url 정보로 자원의 상태를 파악 -> 수정, 협업에 일관적인 개발 프로세스가 가능\n",
    "\n",
    "# GET  :  www.movies.com/movies : 영화를 전부 조회  \n",
    "# GET  :  www.movies.com/movies/:id : 아이디인 영화를 조회\n",
    "# GET  :  www.movies.com/movies/:id : 아이디인 영화를 조회\n",
    "# POST  :  www.movies.com/movies/  : 영화를 생성\n",
    "# PUT :  www.movies.com/movies/ : 영화를 수정\n",
    "# DELETE :  www.movies.com/movies/ : 영화를 삭제\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "s = requests.Session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 예제1\n",
    "r = s.get('https://api.github.com/events')\n",
    "\n",
    "#수신상태 체크\n",
    "r.raise_for_status()\n",
    "\n",
    "\n",
    "# 출력\n",
    "# print(r.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 예제 2\n",
    "# 쿠키 설정(정석 방법)\n",
    "jar = requests.cookies.RequestsCookieJar()\n",
    "\n",
    "# 쿠키삽입 , 저장장소 설정\n",
    "jar.set('name','niceman',domain = 'httpbin.org',path='/cookies')\n",
    "\n",
    "# 요청\n",
    "r = s.get('https://httpbin.org/cookies',cookies=jar)\n",
    "\n",
    "# 출력\n",
    "# print(r.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 예제 3 \n",
    "# 서버가 응답을 줄때까지 5초 기다림 (ex. 외국사이트)\n",
    "r = s.get('https://github.com',timeout=5)\n",
    "\n",
    "# print(r.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 예제 4\n",
    "r = s.post('http://httpbin.org/post', data = {'id':'test77', 'pw':'111'}, cookies=jar)\n",
    "\n",
    "# 출력\n",
    "# print(r.text)\n",
    "# print(r.headers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 예제 5\n",
    "# 요청(POST)\n",
    "payload1 = {'id':'test77' , 'pw':'111'}\n",
    "payload2 = (('id', 'test77') , ('pw', '111'))\n",
    "\n",
    "r = s.post('http://httpbin.org/post', data= payload1)\n",
    "\n",
    "# print(r.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 예제 6(POST)\n",
    "r = s.put('http://httpbin.org/put', data= payload1 )\n",
    "\n",
    "# print(r.text)\n",
    "\n",
    "\n",
    "\n",
    "# 예제 7(DELETE)\n",
    "r = s.delete('http://httpbin.org/delete', data={'id':1})\n",
    "\n",
    "# 출력\n",
    "# print(r.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 예제8()\n",
    "r = s.delete('https://jsonplaceholder.typicode.com/posts/1' )\n",
    "print(r.text)\n",
    "print(r.ok)\n",
    "print(r.headers)\n",
    "\n",
    "\n",
    "\n",
    "s.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Beautiful Soup Selector\n",
    "\n",
    "* HTML 태그 선택자 이해\n",
    "* FIND, FIND_ALL\n",
    "* SELECT, SELECT_ONE\n",
    "* 다양한 DOM 접근 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "<html>\n",
    "    <head>\n",
    "        <title>The Dormouse's story</title>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>this is h1 area</h1>\n",
    "        <h2>this is h2 area</h2>\n",
    "        <p class=\"title\"><b>the Dormouse's story</b></p>\n",
    "        <p class=\"story\">Once upon a time there were three little sistes.\n",
    "            <a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>\n",
    "            <a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a>\n",
    "            <a data-io=\"link3\" href=\"http://example.com/little\" class=\"sister\" id=\"link3\">Title</a>\n",
    "        </p>\n",
    "        <p class=\"story\">\n",
    "            story....\n",
    "        </p>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 1\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "\n",
    "#타입확인\n",
    "# print(type(soup))\n",
    "# print('prettify', soup.prettify())\n",
    "\n",
    "\n",
    "# h1 태그 접근\n",
    "h1 = soup.html.body.h1\n",
    "# print('h1은 ', h1)\n",
    "\n",
    "\n",
    "# p 태그 접근\n",
    "p1 = soup.html.body.p\n",
    "# print(p1)\n",
    "\n",
    "# 다음 태그\n",
    "\n",
    "p2 = p1.next_sibling\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 출력1\n",
    "# print('h1 >>' , h1.string)\n",
    "\n",
    "# 텍스트 출력2  ??\n",
    "# print('p >>' , p1.string)\n",
    "\n",
    "\n",
    "# 다음 엘리먼트 확인\n",
    "print(list(p2.next_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in p2.next_element:\n",
    "#     print(v)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제2(Find, Find_all)\n",
    "\n",
    "soup2 = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "# a 태그 모두 선택\n",
    "link1 = soup2.find_all('a' , limit=2) # limit=2 옵션\n",
    "# print(type(link1))\n",
    "\n",
    "# 리스트 요소 확인\n",
    "# print('links', link1)\n",
    "\n",
    "\n",
    "# 값 접근 방법 ( class , id , string )\n",
    "link2 = soup2.find_all(\"a\", class_='sister') # class\n",
    "# print(link2)\n",
    "\n",
    "link2 = soup2.find_all(\"a\", class_='sister') # id , string \n",
    "# print(link2)\n",
    "\n",
    "link2 = soup2.find_all(\"a\", string=[\"Elsie\"])\n",
    "# print(link2)\n",
    "\n",
    "link2 = soup2.find_all(\"a\", string=[\"Elsie\",\"Title\"])\n",
    "# print(link2)\n",
    "\n",
    "\n",
    "#\n",
    "for t in link2:\n",
    "    pass\n",
    "#     print(t)\n",
    "\n",
    "    \n",
    "# 처음 발견한 a 태그 선택\n",
    "link3 = soup2.find(\"a\")\n",
    "# print()\n",
    "# print(link3)\n",
    "# print(link3.string)\n",
    "# print(link3.text)  # sting 과 text  속성으로 문자를 추출 \n",
    "\n",
    "\n",
    "# 다중 조건\n",
    "link4 = soup2.find(\"a\", {\"class\": \"sister\", 'data-io' : \"link3\"})\n",
    "# print(link4)\n",
    "# print(link4.string)\n",
    "# print(link4.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 방법1. css 선택자 : select / select_one\n",
    "# 방법2. 태그로 접근 : find /  find_all\n",
    "# 예제3 (select, select_one)\n",
    "# 태그 + 클래스 + 자식선택자\n",
    "\n",
    "\n",
    "link5 = soup2.select_one('p.title > b')   # p태그 title클래스 b태그\n",
    "# print(link5)\n",
    "# print(link5.text)\n",
    "\n",
    "\n",
    "link6 =  soup2.select_one('a#link1')\n",
    "print()\n",
    "print(link6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link7 = soup.select_one(\"a[data-io='link3']\")\n",
    "link7.text\n",
    "link7.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선택자에 맞는 전체 선택\n",
    "\n",
    "link8 = soup.select('p.story > a')\n",
    "link8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link9 = soup.select('p.story > a:nth-of-type(2)')\n",
    "link9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link10 = soup.select('p.story')\n",
    "\n",
    "for t in link10:\n",
    "    temp = t.find_all(\"a\")\n",
    "    \n",
    "    if temp:\n",
    "        for v in temp:\n",
    "            print('>>>>>>',v)\n",
    "            print('>>>>>>',v.string)\n",
    "            \n",
    "    else:\n",
    "        print('------',t)\n",
    "        print('------',t.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. 네이버 이미지 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import urllib.parse as rep    # urllib 모듈을 사용할때 편리한 부분이 있으므로 urllib모듈을 사용\n",
    "import urllib.request as req  # retrieve , request\n",
    "from fake_useragent import UserAgent # 페이크유저 에이전트 사용 ex) 네이버 \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# request모듈이 비교적 헤더정보사용하기에 편리하지만...아래의 방법도 알아두는 것이 좋다.\n",
    "\n",
    "# Header 정보 초기화\n",
    "opener = req.build_opener()\n",
    "#User-Agent 정보\n",
    "opener.addheaders = [('User-agent',UserAgent().ie)]\n",
    "#Header 정보 삽입\n",
    "req.install_opener(opener)\n",
    "\n",
    "\n",
    "\n",
    "# 네이버 이미지 기본 URL(크롬개발자 도구)\n",
    "base = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "#검색어\n",
    "quote = rep.quote_plus('호랑이')\n",
    "#URL 완성\n",
    "url = base + quote\n",
    "\n",
    "# 요청 URL 확인\n",
    "print('Request URL : {}'.format(url))\n",
    "\n",
    "# Request\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# 이미지 저장 경로\n",
    "savePath = \"C:/naverimages/\"\n",
    "\n",
    "# 폴더 생성 예외처리(문제 발생 시 프로그램 종료)\n",
    "#권한이 없거나 여러 문제가 있을 때 폴더가 만들어지지않을 수 있다.\n",
    "\n",
    "try: \n",
    "    #기본 폴더가 있는지 체크\n",
    "    if not (os.path.isdir(savePath)):\n",
    "        # 없으면 폴더 생성\n",
    "        os.makedirs(os.path.join(savePath))\n",
    "except OSError as e:\n",
    "    # 에러내용\n",
    "    print('foler creation failed')\n",
    "    print('folder name : {}'.format(e.filename))\n",
    "    \n",
    "    #런타임 에러\n",
    "    raise RuntimeError(\"System Exit!\")\n",
    "else:\n",
    "    #\n",
    "    print(\"folder is created!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  bs4 초기화\n",
    "\n",
    "soup = BeautifulSoup(res,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select 사용\n",
    "img_list = soup.select('div.img_area > a.thumb._thumb > img')\n",
    "\n",
    "for i, img in enumerate(img_list,1):\n",
    "    print()\n",
    "    print()\n",
    "    # 속성확인\n",
    "#     print(img['data-source'],i)\n",
    "\n",
    "    # 저장 파일명 및 경로\n",
    "    fullFileName = os.path.join(savePath,savePath+str(i)+'.png')\n",
    "    print(fullFileName)\n",
    "\n",
    "    #다운로드 요청(URL , 다운로드경로) 다운로드와 동시에 저장할 수 있는 함수 urlretrieve\n",
    "    req.urlretrieve(img['data-source'], fullFileName)\n",
    "    \n",
    "print('download complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. 로그인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "\n",
    "# network - preserv log  체크할 것\n",
    "\n",
    "# Login 정보 (개발자 도구)\n",
    "login_info = {\n",
    "    'redirectUrl': 'http://www.danawa.com/',\n",
    "    'loginMemberType': 'general',\n",
    "    'id': 'hoseong0701',\n",
    "    'password': '*****'    \n",
    "}\n",
    "\n",
    "# Headers 정보\n",
    "headers = {\n",
    "        \"User-Agent\":UserAgent().chrome,\n",
    "        \"Referer\": 'https://auth.danawa.com/login?url=http%3A%2F%2Fwww.danawa.com%2F'\n",
    "}\n",
    "\n",
    "\n",
    "with req.session() as s:\n",
    "    #Request(로그인 시도)\n",
    "    res = s.post('https://auth.danawa.com/login', login_info, headers=headers)\n",
    "    \n",
    "    # 로그인 시도 실패 시 예외\n",
    "    if res.status_code != 200:\n",
    "        raise Exception(\"Login failed!\")\n",
    "        \n",
    "    # 본문 수신 데이터 확인\n",
    "#     print(res.content.decode('utf-8'))\n",
    "\n",
    "    # 로그인 성공 후 세션 정보를 가지고 페이지 이동\n",
    "    res = s.get('https://buyer.danawa.com/order/Order/orderList',headers =headers)\n",
    "    \n",
    "    # Euc-kr(한글 깨질 경우)\n",
    "    # res.encoding = 'euc-kr'\n",
    "    \n",
    "    # 페이지 이동 후 수신 데이터 확인\n",
    "#     print(res.text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "\n",
    "    # 로그인 성공 체크\n",
    "    check_name = soup.find('p',class_='user')\n",
    "    \n",
    "    if check_name is None:\n",
    "        raise Exception('Login failed. wring password')\n",
    "        \n",
    "    # 선택자 사용\n",
    "    info_list = soup.select(\"div.my_info > div.sub_info > ul.info_list > li\")\n",
    "    \n",
    "    # 확인\n",
    "    print(info_list)\n",
    "    \n",
    "    # 이부분에서 재요청 , 파일다운로드 , DB저장 , 파일 쓰기(엑셀)\n",
    "    \n",
    "    # 제목\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"****************My Info************\")\n",
    "    \n",
    "    for v in info_list:\n",
    "        # 속성 메소드 확인\n",
    "        #printer(div(v))\n",
    "        \n",
    "        proc, val = v.find('span').string.strip(), v.find('strong').string.strip()\n",
    "        print('{} : {}'.format(proc,val))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

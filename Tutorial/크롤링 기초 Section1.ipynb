{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. urlretrieve\n",
    "\n",
    "이미지 ,페이지소스 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "\n",
    "# 파일 URL\n",
    "img_url = 'https://c2.staticflickr.com/8/7058/6895007878_6930f04b59_b.jpg'\n",
    "html = 'https://google.com'\n",
    "\n",
    "#다운받을 경로\n",
    "save_path1 = 'C:/tiger.jpg'\n",
    "save_path2 = 'C:/index.html'\n",
    "\n",
    "#예외처리\n",
    "try:\n",
    "    file1, header1 = req.urlretrieve(img_url, save_path1)\n",
    "    file2, header2 = req.urlretrieve(html, save_path2)\n",
    "    \n",
    "except Exception as e:\n",
    "    print('Download failed')\n",
    "    print(e)\n",
    "else:\n",
    "    #Header 정보 출력\n",
    "    print(header1)\n",
    "    print(header2)\n",
    "    \n",
    "    #다운로드 파일 정보\n",
    "    print('Filename1 {}'.format(file1))\n",
    "    print('Filename2 {}'.format(file2))\n",
    "    \n",
    "    #성공\n",
    "    print('Download Succeed')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req\n",
    "from urllib.error import URLError,HTTPError#예외처리 중요 -> 코드의 퀄리티 완성도를 높인다.\n",
    "\n",
    "#다운로드 경로 및 파일명\n",
    "path_list = ['C:/tiger.jpg','C:/index.html']\n",
    "\n",
    "#다운로드 리소스 url\n",
    "target_url = ['https://upload.wikimedia.org/wikipedia/commons/thumb/7/73/Lion_waiting_in_Namibia.jpg/1200px-Lion_waiting_in_Namibia.jpg','http://google.com']\n",
    "\n",
    "for i,url in enumerate(target_url):\n",
    "    \n",
    "    #예외처리\n",
    "    try:\n",
    "        #웹 수신 정보 읽기\n",
    "        response = req.urlopen(url)\n",
    "        \n",
    "        #수신내용\n",
    "        contents = response.read() #메서드 read\n",
    "        \n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "        #상태 정보 중간 출력\n",
    "        print('Header Info-{} : {}'.format(i, response.info())) #메서드 info\n",
    "        print('HTTP Status Code: {}'.format(response.getcode())) #메서드 getcode\n",
    "        print(\"-----------------------------------------------------\")\n",
    "        \n",
    "        with open(path_list[i],'wb') as c:\n",
    "            c.write(contents)\n",
    "        \n",
    "        \n",
    "    except HTTPError as e:\n",
    "        print('Download failed')\n",
    "        print('HTTPError code: ', e.code)\n",
    "    except URLError as e:\n",
    "        print('Download failed')\n",
    "        print('URL Error Reason : ', e.reason)\n",
    "        \n",
    "    #성공\n",
    "    else:\n",
    "        print()\n",
    "        print('Download Succeed.')\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 네이버 메인페이지 기사 링크 크롤링\n",
    "\n",
    "copy selector가 만능은 아니다. 하나의 값이 아닌 전체를 가져올 때는 보통 직접 \"아이디,클래스,태그,속성\"를 검색해보고 접근하여 추출한다.\n",
    "\n",
    "lxml (lxml.html.fromstring)<br>\n",
    "requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "import requests\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    네이버 메인뉴스 스탠드 스크래핑 메인함수\n",
    "    \"\"\"\n",
    "    # 스크래핑 대상 URL\n",
    "    response = requests.get(\"https://www.naver.com\") # get방식\n",
    "    \n",
    "    #신문사 링크 리스트 획득\n",
    "    urls = scrape_news_list_page(response)\n",
    "    \n",
    "    #결과출력\n",
    "    for url in urls:\n",
    "        #url출력\n",
    "        print(url)\n",
    "        #파일쓰기\n",
    "        #생략\n",
    "        \n",
    "        \n",
    "def scrape_news_list_page(response):\n",
    "    #URL리스트 선언\n",
    "    urls = []\n",
    "    \n",
    "    #태그 정보 문자열 저장\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    \n",
    "    for  a in root.cssselect('.api_list .api_item a.api_link'):\n",
    "        #링크\n",
    "        url = a.get('href')\n",
    "        urls.append(url)\n",
    "    \n",
    "    return urls\n",
    "    \n",
    "    \n",
    "#스크래핑 시작\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 완성된 스크래핑 코드 작성\n",
    "\n",
    "* Session 이용해서 로그인 정보를 얻을 수 있다. 연결의 흐름을 유지하여 일정된 서비스를 제공할 수 있다. <br>\n",
    "ex) 로그인 활성화 상태 <- Session 정보 사용\n",
    "<br>\n",
    "\n",
    "* xpath로 값 찾기 <br><br>\n",
    "\n",
    "* 딕셔너리 형태 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml.html import fromstring, tostring\n",
    "import requests\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    네이버 메인뉴스 스탠드 스크래핑 메인함수\n",
    "    \"\"\"\n",
    "    #세션 사용\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # 스크래핑 대상 URL\n",
    "    response = requests.get(\"https://www.naver.com\") # get방식\n",
    "    \n",
    "    #신문사 링크 딕셔너리 획득\n",
    "    urls = scrape_news_list_page(response)\n",
    "    \n",
    "    #결과출력\n",
    "    for name, url in urls.items():\n",
    "        \n",
    "        #url출력\n",
    "        print(name, url)\n",
    "        \n",
    "        #파일쓰기\n",
    "        #생략\n",
    "        \n",
    "        \n",
    "def scrape_news_list_page(response):\n",
    "    \n",
    "    #URL리스트 선언\n",
    "    urls = {}\n",
    "    \n",
    "    #태그 정보 문자열 저장\n",
    "    root = fromstring(response.content)\n",
    "    \n",
    "    for  a in root.xpath('//ul[@class=\"api_list\"]/li[@class=\"api_item\"]/a[@class=\"api_link\"]'):\n",
    "        \n",
    "        #a 구조확인\n",
    "#         print(a)\n",
    "        \n",
    "        # a의 문자열 출력\n",
    "#         print(tostring(a,pretty_print=True))\n",
    "    \n",
    "        name, url = extract_contents(a)\n",
    "        # 딕셔너리 삽입\n",
    "        urls[name] =  url\n",
    "        \n",
    "    return urls\n",
    "\n",
    "\n",
    "def extract_contents(dom):\n",
    "    #링크주소\n",
    "    link = dom.get('href')\n",
    "    \n",
    "    #신문사명\n",
    "    name = dom.xpath('./img')[0].get('alt') #xpath('/img')\n",
    "    return  name , link\n",
    "\n",
    "    \n",
    "#스크래핑 시작\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 사이트 요청 정보 확인\n",
    "\n",
    "* encar(엔카) 사이트 정보 수신\n",
    "\n",
    "* Get 파라미터 요청\n",
    "\n",
    "* 수신 데이터 디코딩(Decoding)\n",
    "\n",
    "* 요청 URL 정보분석\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request \n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# 기본 요청1(encar)\n",
    "url = \"http://www.encar.com\"\n",
    "\n",
    "mem = urllib.request.urlopen(url)\n",
    "\n",
    "# 여러정보\n",
    "print('type : {}'.format(type(mem)))\n",
    "print('getrul: {}'.format(mem.geturl()))  #자동으로 리다이렉트된 수신 사이트주소를 리턴\n",
    "print('status: {}'.format(mem.status)) # or getcode\n",
    "print('headers: {}'.format(mem.getheaders())) #수신된 헤더 정보\n",
    "print('getcode: {}'.format(mem.getcode())) # or tatus\n",
    "print('read: {}'.format(mem.read(100).decode('utf-8'))) # 바이트수 지정 ,바이트수가 너무크면 에러가 날 수 있다.\n",
    "print('parse: {}'.format(urlparse('http://www.encar.co.kr?id=test&pw=1111'))) #전체정보\n",
    "print('parse: {}'.format(urlparse('http://www.encar.co.kr?id=test&pw=1111').query)) # parse를 사용하여 쿼리정보만 확인\n",
    "\n",
    "#get방식은 주요정보가 노출이 되기때문에 공개적인 서비스(ex.신문사)에서 사용된다.\n",
    "#중요하거나 원문 데이터는 post방식을 사용\n",
    "# 위의 정보를 사용하여 예외처리를 할 수있기때문에 urllib를 사용\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본 요청2(ipifi)\n",
    "API = 'https://api.ipify.org'\n",
    "\n",
    "# Get 방식 Parameter\n",
    "values = {\n",
    "    'format' : 'json'\n",
    "#     'format' : 'jsonP'\n",
    "#     'format' : 'text'\n",
    "}\n",
    "\n",
    "print('before param : {}'.format(values))\n",
    "params = urllib.parse.urlencode(values)\n",
    "print('after param : {}'.format(values))\n",
    "\n",
    "\n",
    "# 요청 URL 생성\n",
    "URL = API + \"?\" + params   #수신요청한 나의 아이피주소\n",
    "\n",
    "\n",
    "# 수신 데이터 읽기\n",
    "data = urllib.request.urlopen(URL).read()\n",
    "\n",
    "# 수신데이터 디코딩\n",
    "text = data.decode('UTF-8') # python3버젼은 기본적으로 utf-8 이기 떄문.\n",
    "print('response: {}'.format(text)) # 나의 아이피가 서버에서 내려준 정보를 수신했다는 것을 나타냄\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 행정안전부 사이트 RSS 데이터 수신\n",
    "\n",
    "* RSS란?\n",
    "\n",
    "https://www.mois.go.kr/gpms/view/jsp/rss/rss.jsp?ctxCd=1001 <br>ctxCd=1001<br>\n",
    "https://www.mois.go.kr/gpms/view/jsp/rss/rss.jsp?ctxCd=1013 <br>ctxCd=1013<br>\n",
    ".\n",
    ".\n",
    "\n",
    "\n",
    "* 반복문을 활용한 연속 요청\n",
    "\n",
    "* 요청 URL 정보 분석\n",
    "\n",
    "* 수신 XML 데이터 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "# 행정 안전부 : https://www.mois.go.kr\n",
    "# 행정 안전부 RSS API URL\n",
    "\n",
    "API = 'https://www.mois.go.kr/gpms/view/jsp/rss/rss.jsp'\n",
    "\n",
    "params = []\n",
    "\n",
    "for num in [1001,1012,1013]:\n",
    "    \n",
    "    params.append(dict(ctxCd=num))\n",
    "    \n",
    "# 중간 확인\n",
    "print(params)\n",
    "\n",
    "\n",
    "#연속해서 4회 요청\n",
    "for c in params:\n",
    "    \n",
    "    # 파라미터 출력\n",
    "    # print(c)\n",
    "    \n",
    "    # URL 인코딩\n",
    "    params = urllib.parse.urlencode(c)  # 딕셔너리를 url 형태로 변환  { a : b }  ->  a = b\n",
    "    \n",
    "    #URL완성\n",
    "    url = API + '?' + params\n",
    "    \n",
    "    # URL 출력\n",
    "    print('url :',url)\n",
    "    \n",
    "    # 요청\n",
    "    res_data = urllib.request.urlopen(url).read()\n",
    "#     print(res_data)\n",
    "\n",
    "    # 수신 후 디코딩\n",
    "    contents = res_data.decode(\"UTF-8\")\n",
    "    \n",
    "    # 출력\n",
    "    print(\"*\"*100)\n",
    "    print(\"*\"*100)\n",
    "    print(\"*\"*100)\n",
    "    print(\"*\"*100)\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 주식 정보 가져오기\n",
    "\n",
    "특정 사이트에서는 코드로 요청을 하는지 브라우져에서 요청을 하는지 구분을 하고 제한을 걸 수 있다.<br>\n",
    "따라서 이러한 제한을 최소화하기 위해 코드로 직접 헤더정보를 입력하여 보내야 원하는 데이터를 수신할 수 있다.\n",
    "\n",
    "\n",
    "* 비동기 통신\n",
    "\n",
    "처음에 랜더링될때는 원하는 없는 정보가 제공되고 추가적으로 후에 정보를 채워넣는 통신방식\n",
    "\n",
    "따라서 요청했을 때 원하는 정보가 없을때는 구글 개발자도구의 네트워크탭에서 다른 url을 살펴본다.\n",
    "\n",
    "step1. preview 또는 response정보를 확인하여 원하는 정보가 있는지 본다. <br><br>\n",
    "step2. request Header 에서 브라우져로 인식시키기 위한 필요 정보를 확인한다.\n",
    "ex) referer:  , user-agent:<br>\n",
    "\n",
    "step3. fake-useragent 사용 , Header 정보삽입<br>\n",
    "pip install fake-useragent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  # 제공되는 데이터 타입이 json일때 사용\n",
    "\n",
    "import urllib.request as req\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "\n",
    "# Fake Header정보 (가상으로 User-agent생성)\n",
    "ua = UserAgent()\n",
    "# print(ua.ie)\n",
    "# print(ua.msie)\n",
    "# print(ua.chrome)\n",
    "# print(ua.safari)\n",
    "# print(ua.random)\n",
    "\n",
    "\n",
    "# 헤더 정보\n",
    "headers = {\n",
    "    'User-agent':ua.ie,\n",
    "    'referer':'https://finance.daum.net/'\n",
    "}\n",
    "\n",
    "\n",
    "# 다음 주식 요청 URL\n",
    "url = 'https://finance.daum.net/api/search/ranks?limit=10'\n",
    "\n",
    "\n",
    "# 요청\n",
    "res = req.urlopen( req.Request(url, headers=headers )).read().decode('UTF-8')\n",
    "\n",
    "\n",
    "#응답 데이터 확인(Json Data)\n",
    "# print('res',res)\n",
    "\n",
    "\n",
    "\n",
    "#응답 데이터 str -> json 변환 및 data 값 출력\n",
    "rank_json = json.loads(res)['data']\n",
    "\n",
    "\n",
    "#중간확인\n",
    "# print('중간확인:',rank_json,'\\n')\n",
    "\n",
    "\n",
    "for elm in rank_json:\n",
    "    print('순위 : {}, 금액 : {}, 회사명 : {}'.format(elm['rank'],elm['tradePrice'],elm['name']))\n",
    "    #파일(csv, 엑셀 , txt) 저장 및 db 저장\n",
    "    #code\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
